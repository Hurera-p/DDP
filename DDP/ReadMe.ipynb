{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Dask is a parallel computing framework that extends Python's standard data science libraries like Pandas, NumPy, and scikit-learn to handle larger-than-memory datasets and distributed computation. Here is how it works in our project",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**1) Parallel Reading of Large Dataset**\n- Code: dd.read_csv(dataset_path)\n- Dask reads the CSV file in parallel by splitting it into chunks (partitions). Each partition is processed independently, enabling efficient use of memory and computation resources. Unlike Pandas, which loads the entire dataset into memory, Dask processes only chunks that fit into memory, making it suitable for handling large datasets.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**2) Lazy Evaluation**\n- Operations like groupby, sum, count, and mean in Dask are lazy.\n- When you call ddf.groupby(\"payment_type\")[\"total_amount\"].sum(), Dask builds a computation graph but doesnâ€™t execute it immediately. The computation is triggered only when you call .compute(). This approach:\n  - Reduces memory usage since computations are performed on demand.\n  - Allows Dask to optimize the execution by combining tasks and avoiding redundant computations.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**3) Distributed Data Processing**\n- Your dataset is divided into partitions across cores. Each operation, such as groupby, is executed on these smaller partitions independently. For example:\n  - Summing total revenue per payment type: Each partition calculates the sum for its subset of data, and the results are combined in a final aggregation step.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**4) Scalable Aggregations**\n- Aggregation functions like .sum() or .mean() operate on intermediate results from each partition.\n  - For instance, mean is computed using partial means and counts from each partition, avoiding the need to load all data at once.\n  - Take this dataset for instance [10,20,30,40,50,60] \n  - Divide this dataset into 3 \n  - Partition 1: Sum = 30, Count = 2\n  - Partition 2: Sum = 70, Count = 2\n  - Partition 3: Sum = 110, Count = 2\n  - A = total of Sum, B = total of count \n  - A/B => 210/6 => 35 \n- This process ensures scalability for large datasets.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**5) Efficiency**\n- Dask's multi-threading or distributed execution model allows it to parallelize operations across multiple CPU cores or even machines, leading to faster computation than single-threaded libraries like Pandas for large datasets.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Benefits of Using Dask in our Project**\n1. Handles Large Datasets: Works seamlessly even if the dataset is larger than available RAM.\n2.\tFaster Execution: Parallel processing accelerates operations compared to Pandas.\n3.\tFlexible Deployment: Can scale from a single machine to a distributed cluster if needed.\n4.\tEfficient Resource Use: Processes only chunks of data at a time, reducing memory overhead.\n",
      "metadata": {}
    }
  ]
}